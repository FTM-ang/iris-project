# -*- coding: utf-8 -*-
"""Iris macine learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zP6dWN34nT4iA7EuO2N-qDJ8SI-xut_W
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)

corr_matrix = X.corr()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation matrix for iris dataset")
plt.show()

"""Noticing petal lenght and petal width has strong correlation, PCA can help reduce this redundancy"""

from sklearn.decomposition import PCA

pca = PCA(n_components=4)
X_pca = pca.fit_transform(X)


explained_variance = pca.explained_variance_ratio_

for i, ratio in enumerate(explained_variance):
    print(f"component: {i+1}: {ratio:.2f}")


plt.figure(figsize=(6, 4))
plt.bar(range(1, 5), explained_variance, alpha=0.7, color='teal')
plt.xlabel('PCA components:')
plt.ylabel('explaind variance')
plt.grid(True)
plt.show()

"""Applied PCA with 4 components (same as original features) to identify how much variance each one explains.
The first component captured the majority of the variance, followed by the second.
The remaining two components had minimal impact, so two components are likely enough for dimensionality reduction.
"""

pca_optimal = PCA(n_components=2)
X_pca_optimal = pca_optimal.fit_transform(X)

plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    X_pca_optimal[:, 0], X_pca_optimal[:, 1],
    cmap='viridis', edgecolor='k', alpha=0.7
)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Projection (2 Components) of Iris Dataset")
plt.legend(*scatter.legend_elements(), title="Classes")
plt.grid(True)
plt.show()

"""better Color the data points based on their target classes for better visualization."""

y = iris.target
target_names = iris.target_names

plt.figure(figsize=(8, 6))
for i, target_name in enumerate(target_names):
    plt.scatter(
        X_pca[y == i, 0], X_pca[y == i, 1],
        label=target_name, alpha=0.7, edgecolor='k'
    )

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection (2 Components) of Iris Dataset')
plt.legend()
plt.grid(True)
plt.show()

"""PCA projection shows clear separation between iris classes in 2D space.
This confirms that dimensionality reduction preserves class variance well.
"""